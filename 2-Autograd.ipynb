{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://discuss.pytorch.org/uploads/default/original/2X/3/35226d9fbc661ced1c5d17e374638389178c3176.png\" width=\"400\" style=\"margin: 50px auto; display: block; position: relative; left: -30px;\" />\n",
    "</div>\n",
    "\n",
    "<!--NAVIGATION-->\n",
    "# < [Basics](1-Basics.ipynb) | Autograd | [Optimization](3-Optimization.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "Automatic differentation (autodiff) is a key feature of PyTorch.\n",
    "PyTorch can differentiate the outcome of any computation with respect to its inputs. You don't need to compute the gradients yourself. This allows to to express and optimize complex models without worrying about correctly differentiating the model.\n",
    "\n",
    "We will start by discussing a little bit of the math behind autodiff. We then cover PyTorch's `.backward()` method that does everything automatically for you. Finally, we have a quick look under the hood to see how PyTorch does its magic.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "#### 1. [Usage in PyTorch](#Usage-in-PyTorch)\n",
    "#### 2. [Differentiation fundamentals](#Differentiation-fundamentals)\n",
    "#### 3. [Advanced topics](#Advanced-topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Usage in PyTorch\n",
    "\n",
    "Let's start with a function that is easy to differentiate: $f(x) = 3 x^2 + 4$. It is easy to see that the derivative $\\frac{d}{dx}f(x)$ equals $6 \\cdot x$. PyTorch can compute this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x with some value\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(\"x = {}\".format(x))\n",
    "\n",
    "# Execute f(x)\n",
    "y = 3 * x**2  + 4\n",
    "print(\"y = {}\".format(y))\n",
    "\n",
    "# Compute the gradient of y with respect to all variables that have 'requires_grad' turned on\n",
    "y.backward()\n",
    "\n",
    "# Checking the result\n",
    "computed_gradient = x.grad\n",
    "print(\"PyTorch computed the gradient {}\".format(computed_gradient))\n",
    "print(\"We would expect it to be {}\".format(6 * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Differentiation fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this was easy enough to do by ourselves, differentiating more complex expressions takes time and is prone to human errors. Computers are much better at doing this correctly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "\n",
    "To differentiate an expression like $3x^2 + 4$ in a very methodical way, you can interpret is as a sequence of basic operations. Something like \n",
    "\n",
    "$$y = f(x) = \\text{plus4}(\\text{times3}(\\text{square}(x))).$$\n",
    "\n",
    "The gradient of $y$ with respect to $x$ can now be computed systematically using the chain rule:\n",
    "\n",
    "$$f'(x) = \\text{plus4}'(\\cdots) \\, \\text{times3}'(\\cdots) \\, \\text{square}'(x).$$\n",
    "\n",
    "### Computation graph\n",
    "\n",
    "When you indicate with `requires_grad` that you will need a gradient of some output w.r.t. some input `x`, PyTorch will track any computations that are based on `x`. This ‘history’ is called the **computation graph**. For our simple polynomial, it would look like this:\n",
    "\n",
    "![forward pass](figures/computation-graph-1.svg)\n",
    "\n",
    "You can explore how PyTorch keeps track of history by inspecting the `tensor.grad_fn` argument:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions[0][0])\n",
    "print(y.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value has a `grad_fn` corresponding to the operation that produced the value. \n",
    "Each operation's `grad_fn` points to its inputs through `next_functions`.\n",
    "For each input, `next_functions` contains a tuple of the input's `grad_fn` and, if the operation had multiple outputs, an index of the relevant output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, the final `add` operation has two inputs:\n",
    "# - The first is the output of `multiplication`.\n",
    "# - The second is a constant `4` for which we don't require a gradient.\n",
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "After you have computed the output $y=3x^2+4$, you can call `y.backward()` to compute the gradients of $y$ with respect to all inputs that have `requires_grad`. You can see how the chain rule is naturally executed backwards:\n",
    "\n",
    "![backward pass](figures/computation-graph-2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing old gradients\n",
    "\n",
    "Let's revisit the previous basic example, but call `.backward()` multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x with some value\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 3 * x**2  + 4\n",
    "\n",
    "y.backward()\n",
    "print(\"PyTorch computed the gradient {}\".format(x.grad))\n",
    "\n",
    "# !ERROR!\n",
    "y = 3 * x**2  + 4\n",
    "y.backward()\n",
    "print(\"PyTorch computed the gradient {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the second time, the gradient computed is too large. This is because `.backward()` __accumulates__ the gradients. If you want fresh values, you need to set `x.grad` to zero before you call `.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x with some value\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 3 * x**2  + 4\n",
    "\n",
    "y.backward()\n",
    "print(\"PyTorch computed the gradient {}\".format(x.grad))\n",
    "\n",
    "x.grad = None  # FIX: Delete the previously computed gradient\n",
    "y = 3 * x**2  + 4\n",
    "y.backward()\n",
    "print(\"PyTorch computed the gradient {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping history tracking with `torch.no_grad()`\n",
    "\n",
    "*Advanced*\n",
    "\n",
    "After you trained a model, you just want to use it without computing gradients.\n",
    "Building a computation graph for every operation would be wasteful if you don't need it.\n",
    "Therefore, you can skip these operations by wrapping your code with the `with torch.no_grad():` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x.requires_grad : \", x.requires_grad)\n",
    "\n",
    "y = (x ** 2)\n",
    "print(\"y.requires_grad : \", y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = (x ** 2)\n",
    "    print(\"y.requires_grad : \", y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaves vs Nodes\n",
    "\n",
    "*Advanced*\n",
    "\n",
    "PyTorch's autograd mechanism differentiates between two types of tensors:\n",
    "- __node variables__ are the result of a pytorch operation\n",
    "- __leaf variables__ are directly created by a user\n",
    "\n",
    "Later in the tutorial, we will use the `.is_leaf` property to differentiate between the two types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "B = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True) + 2  # B is the result of an operation (+)\n",
    "C = 5 * A  # C is the result of an operation (*)\n",
    "print(\"A.is_leaf :\", A.is_leaf)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "print(\"C.is_leaf :\", C.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping history with `.detach()`\n",
    "\n",
    "*Advanced*\n",
    "\n",
    "Some tensors are computed from others, but you may want to consider them as __new leaf variables__, i.e. treat them as constants without computation history. For that, you can use the `.detach()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(1,2, requires_grad=True)\n",
    "B = A.mean()\n",
    "\n",
    "print(\"B : \", B)\n",
    "print(\"B.grad_fn :\", B.grad_fn)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "\n",
    "C = B.detach()\n",
    "print(\"\\n-- C = B.detach() -- \\n\")\n",
    "\n",
    "print(\"C : \", C)\n",
    "print(\"C.grad_fn :\", C.grad_fn)\n",
    "print(\"C.is_leaf :\", C.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiating w.r.t. intermediate values: `.retain_grad()`\n",
    "\n",
    "*Advanced*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the backward pass, Autograd computes the gradient of the output with respect to every intermediate variables. However, by default, only gradients of variables that were **created by the user** (leaf) and have the __`requires_grad` property to True__ are saved.\n",
    "\n",
    "Indeed, most of the time when training a model you only need the gradient of a loss w.r.t. to your model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "B.retain_grad()  # <----- This line let us have access to gradient wrt. B after the backward pass\n",
    "C = B.mean()\n",
    "\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "# < [Basics](1-Basics.ipynb) | Autograd | [Optimization](3-Optimization.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
