{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://discuss.pytorch.org/uploads/default/original/2X/3/35226d9fbc661ced1c5d17e374638389178c3176.png\" width=\"400\" style=\"margin: 50px auto; display: block; position: relative; left: -30px;\" />\n",
    "</div>\n",
    "\n",
    "<!--NAVIGATION-->\n",
    "# < [Optimization](3-Optimization.ipynb) | Modules | [CNN](5-CNN.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Modules\n",
    "\n",
    "Modules are a way to build re-usable model components and to manage model parameters.\n",
    "PyTorch has many built-in modules for common operations like convolutions, recurrent neural networks, max pooling, common activation functions, etc.\n",
    "You can also build your own modules.\n",
    "\n",
    "We finish this notebook by building a small neural network to perform image classification on MNIST.\n",
    "\n",
    "### Table of contents\n",
    "#### 1. [Modules](#Modules)\n",
    "#### 2. [Building a Neural Network](#Building-a-Neural-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All modules in PyTorch inherit from the `torch.nn.Module` base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules manage parameters\n",
    "\n",
    "They \n",
    "- automatically keep track of the parameters in your model.\n",
    "- simplify saving/loading of your model.\n",
    "- help you reset gradients (with `model.zero_grad()`)\n",
    "- simplify moving all parameters to the gpu (with `model.cuda()`)\n",
    "\n",
    "The model parameter's are represented by `torch.nn.Parameter` objects.\n",
    "A `Parameter` is a tensor with `requires_grad` to `True` by default, and which is automatically added to the list of parameters when used within a model. Let's have a look at the documentation ([torch.nn.Paramter](https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.nn.Parameter.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Conv1d` module has two parmeters: `weight` as `bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = torch.nn.Conv1d(5, 2, 3)\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Create an instance of the model\n",
    "# each instance has its own parameters\n",
    "linear_regression_model = torch.nn.Linear(5, 2)  # 5 input dimensions, 2 output dimensions\n",
    "\n",
    "# Example batch of data_points\n",
    "#   - models operate on batches of multiple examples together\n",
    "batch_size = 1\n",
    "x = torch.randn(batch_size, 5)\n",
    "y = torch.randn(batch_size, 2)\n",
    "\n",
    "print(\"x = {}\\ny = {}\".format(x, y))\n",
    "\n",
    "# (2) Run the model\n",
    "predicted_y = linear_regression_model(x)\n",
    "print(\"predicted y = {}\".format(predicted_y))\n",
    "\n",
    "# Compute a loss\n",
    "loss = torch.sum((y - predicted_y)**2)\n",
    "\n",
    "# (3) Compute a gradient w.r.t. all model parameters\n",
    "linear_regression_model.zero_grad()  # clear previous gradient\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "print(linear_regression_model.weight.grad)\n",
    "print(linear_regression_model.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Composing modules with `torch.nn.Sequential`\n",
    "\n",
    "If your model is a simple chain of other modules, you can compose them using `torch.nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 2),\n",
    ")\n",
    "\n",
    "# Run the model:\n",
    "neural_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module parameters\n",
    "\n",
    "You can inspect your network's parameters using `.parameters()` or `.named_parameters()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, tensor in neural_net.named_parameters():\n",
    "    print(\"{:10s} shape={}\".format(param, tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Custom modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A module has to implement two functions:\n",
    "\n",
    "- the `__init__` function, where you define all the layers that have learnable parameters. In the `__init__` function, you are just specifying each layer and not how it is connected to others, so it does not need to be in order of execution. Since your model's submodules and parameters are instantiated in the `__init__` function, PyTorch knows that they exist and registers them.  \n",
    "Also, don't forget to always call the `super()` method.  \n",
    "\n",
    "\n",
    "- the `forward` function, which is the method that defines what has to be executed during the forward pass and especially how the layers are connected. This is where you call the layers that you defined inside the `__init__` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most basic form of a custom module:\n",
    "class MySuperSimpleModule(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define sub-modules or parameters\n",
    "        # torch.nn.Module takes care of adding their parameters to the new module\n",
    "        self.linear = torch.nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the print function to list a model's submodules and parameters defined inside `init`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySuperSimpleModule(input_size=20, num_classes=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `model.parameters()` to get the list of parameters of your model automatically inferred by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():  # Here we use a sligtly different version of the parameters() function\n",
    "    print(name, \":\\n\", p)                 # which also returns the parameter name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__<br>\n",
    "Implement a simple multilayer perceptron with two hidden layers and the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/ledell/sldm4-h2o/master/mlp_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the following number of neurons in each layer:\n",
    "\n",
    "- Input-size: *input_size*\n",
    "- 1st hidden layer: 75\n",
    "- 2nd hidden layer: 50\n",
    "- Output layer: *num_classes*\n",
    "\n",
    "We use `ReLU`s as ‘activation functions’ inbetween each pair of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need some PyTorch NN modules - Find them in the [PyTorch doc](https://pytorch.org/docs/master/nn.html) (especially nn.Linear)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # provides some helper functions like Relu's, Sigmoids, Tanh, etc.\n",
    "\n",
    "class MyMultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MyMultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(input_size, 75)\n",
    "        self.linear_2 = # <YOUR CODE>\n",
    "        self.linear_3 = torch.nn.Linear(50, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear_1(x))\n",
    "        out = # <YOUR CODE>\n",
    "        out = # <YOUR CODE>\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now feed an input to your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(16, 784)  # the first dimension is reserved for the 'batch_size'\n",
    "out = model(x)  # equivalent to model.forward(x)\n",
    "out[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training the model\n",
    "\n",
    "Most of the functions to train a model follow a similar pattern in PyTorch.\n",
    "In most of the cases in consists of the following steps:\n",
    "- Loop over data (in batches)\n",
    "- Create a prediction (forward pass)\n",
    "- Clear previous gradients (!)\n",
    "- Compute gradients (backward pass)\n",
    "- Parameter update (using an optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, data_loader, device):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define the Loss function and Optimizer that you want to use\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # NOTE: model.parameters()\n",
    "    \n",
    "    # Outter training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Inner training loop\n",
    "        cum_loss = 0\n",
    "        for (inputs, labels) in data_loader:\n",
    "            # Prepare inputs and labels for processing by the model (e.g. reshape, move to device, ...)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # original shape is [batch_size, 28, 28] because it's an image of size 28x28\n",
    "            inputs = inputs.view(-1, 28*28)\n",
    "\n",
    "            # Do Forward -> Loss Computation -> Backward -> Optimization\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cum_loss += loss.item()\n",
    "        print(\"Epoch %d, Loss=%.4f\" % (epoch+1, cum_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- We can use the `.to` function on the model directly. Indeed, since PyTorch knows all the model parameters, it can put all the parameters on the correct device.\n",
    "- We use `model.parameters()` to get all the parameters of the model and we can instantiate an optimizer that will optimize these parameters `torch.optim.SGD(model.parameters())`.\n",
    "- To apply the forward function of the module, we write `model(input)`. In most cases, `model.forward(inputs)` would also work, but there is a slight difference : PyTorch allows you to register hook functions for a model that are automatically called when you do a forward pass on your model. Using `model(input)` will call these hooks and then call the forward function, while using `model.forward(inputs)` will just silently ignore them.\n",
    "\n",
    "Do you see the convenience of Modules?\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "PyTorch comes with a lot of predefined loss functions :\n",
    "- L1Loss\n",
    "- MSELoss\n",
    "- CrossEntropyLoss\n",
    "- NLLLoss\n",
    "- PoissonNLLLoss\n",
    "- KLDivLoss\n",
    "- BCELoss\n",
    "- ...\n",
    "\n",
    "Check out the [PyTorch Documentation](https://pytorch.org/docs/master/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The MNIST digit-classification dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST](figures/mnist.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the training and test images. MNIST is a widely used dataset, therefore the torchvision package provides simple functionalities to load images from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Batcher)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `Dataset` and `Dataloaders` are classes that can help to quickly define how to access and iterate over your data. This is specially interesting when your data is distributed over several files (for instance, if you have several images in some directory structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyMultilayerPerceptron(input_size=784, num_classes=10)\n",
    "num_epochs = 5\n",
    "\n",
    "train(model, num_epochs, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing model performance \n",
    "This function loops over another `data_loader` (usually containing test/validation data) and computes the model's accuracy on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader, device):\n",
    "    with torch.no_grad(): # during model evaluation, we don't need the autograd mechanism (speeds things up)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)     \n",
    "            inputs = inputs.view(-1, 28*28)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            correct += (predicted.cpu() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, test_loader, device)  # look at: accuracy(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get an accuracy of ~97.9%. Can you improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_loaded = torch.load(\"my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear_3.bias, my_model_loaded.linear_3.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This intro to modules used [this medium post](https://medium.com/deeplearningbrasilia/deep-learning-introduction-to-pytorch-5bd39421c84) as a resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "# < [Optimization](3-Optimization.ipynb) | Modules | [CNN](5-CNN.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
